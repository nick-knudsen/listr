{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97efd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import duckdb as dk\n",
    "\n",
    "con = dk.connect(\"data/vermont.duckdb\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9b9a1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sightings_df = dd.read_csv('data/ebd_US-VT_smp_relJul-2025.txt', \n",
    "                 sep='\\t', \n",
    "                 usecols=['GLOBAL UNIQUE IDENTIFIER', 'LAST EDITED DATE', 'TAXONOMIC ORDER', 'CATEGORY', 'COMMON NAME', 'SCIENTIFIC NAME', 'OBSERVATION COUNT', 'STATE', 'COUNTY', 'COUNTY CODE', 'LOCALITY', 'LOCALITY ID', 'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE', 'TIME OBSERVATIONS STARTED', 'OBSERVER ID', 'SAMPLING EVENT IDENTIFIER', 'OBSERVATION TYPE', 'DURATION MINUTES', 'EFFORT DISTANCE KM', 'NUMBER OBSERVERS', 'ALL SPECIES REPORTED', 'GROUP IDENTIFIER'],\n",
    "                 blocksize=25e6,\n",
    "                 na_values={'OBSERVATION COUNT': 'X'},\n",
    "                 dtype={\n",
    "                        'GLOBAL UNIQUE IDENTIFIER': 'string',\n",
    "                        'LAST EDITED DATE': 'string',\n",
    "                        'TAXONOMIC ORDER': 'UInt32',\n",
    "                        'CATEGORY': 'category',\n",
    "                        'COMMON NAME': 'category',\n",
    "                        'SCIENTIFIC NAME': 'category',\n",
    "                        'OBSERVATION COUNT': 'UInt32',\n",
    "                        'STATE': 'category',\n",
    "                        'COUNTY': 'category',\n",
    "                        'COUNTY CODE': 'category',\n",
    "                        'LOCALITY': 'string',\n",
    "                        'LOCALITY ID': 'string',\n",
    "                        'LOCALITY TYPE': 'category',\n",
    "                        'LATITUDE': 'float64',\n",
    "                        'LONGITUDE': 'float64',\n",
    "                        'OBSERVATION DATE': 'period[D]',\n",
    "                        'TIME OBSERVATIONS STARTED': 'string',\n",
    "                        'OBSERVER ID': 'string',\n",
    "                        'SAMPLING EVENT IDENTIFIER': 'string',\n",
    "                        'OBSERVATION TYPE': 'category',\n",
    "                        'DURATION MINUTES': 'UInt16',\n",
    "                        'EFFORT DISTANCE KM': 'Float32',\n",
    "                        'NUMBER OBSERVERS': 'UInt8',\n",
    "                        'ALL SPECIES REPORTED': 'boolean',\n",
    "                        'GROUP IDENTIFIER': 'string',\n",
    "\n",
    "                        }\n",
    "                )\n",
    "\n",
    "sightings_df['GLOBAL UNIQUE IDENTIFIER'] = sightings_df['GLOBAL UNIQUE IDENTIFIER'].str.extract(r'(\\d+)$')[0].astype('Int64')\n",
    "sightings_df['SAMPLING EVENT IDENTIFIER'] = sightings_df['SAMPLING EVENT IDENTIFIER'].str.extract(r'(\\d+)$')[0].astype('Int64')\n",
    "sightings_df['LOCALITY ID'] = sightings_df['LOCALITY ID'].str.extract(r'(\\d+)$')[0].astype('Int64')\n",
    "sightings_df['GROUP IDENTIFIER'] = sightings_df['GROUP IDENTIFIER'].str.extract(r'(\\d+)$')[0].astype('Int64')\n",
    "sightings_df['LAST EDITED DATE'] = dd.to_datetime(sightings_df['LAST EDITED DATE'], errors='coerce')\n",
    "sightings_df = sightings_df.categorize(columns=['COMMON NAME', 'SCIENTIFIC NAME', 'COUNTY CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "42081e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_parquet(sightings_df, 'data/VT_observations.parquet', engine=\"pyarrow\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4805f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sightings_df = dd.read_parquet('data/VT_observations.parquet', engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "825d15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hotspot_sightings_df = sightings_df[\n",
    "      (sightings_df['LOCALITY TYPE'] == 'H')\n",
    "    & (sightings_df['CATEGORY'] == 'species')\n",
    "    & (sightings_df['ALL SPECIES REPORTED'])\n",
    "]\n",
    "\n",
    "# print(\"Num sightings: \", len(complete_hotspot_sightings_df))\n",
    "# print(\"Num group sightings: \", complete_hotspot_sightings_df['GROUP IDENTIFIER'].count().compute())\n",
    "# print(\"Num solo sightings: \", complete_hotspot_sightings_df['GROUP IDENTIFIER'].isna().sum().compute())\n",
    "\n",
    "# REMOVE DUPLICATE ROWS FROM GROUP CHECKLISTS \n",
    "\n",
    "# not sure why this logic isn't working, fix later, workaround below\n",
    "# unique_complete_hotspot_sightings_df = complete_hotspot_sightings_df.drop_duplicates(subset=['GROUP IDENTIFIER'], split_every=False)\n",
    "\n",
    "solo_sightings_df = complete_hotspot_sightings_df[complete_hotspot_sightings_df['GROUP IDENTIFIER'].isna()]\n",
    "group_sightings_df = complete_hotspot_sightings_df[complete_hotspot_sightings_df['GROUP IDENTIFIER'].notnull()]\n",
    "group_sightings_df = group_sightings_df.drop_duplicates(subset=['GROUP IDENTIFIER', 'COMMON NAME'])\n",
    "\n",
    "unique_complete_hotspot_sightings_df = dd.concat([solo_sightings_df, group_sightings_df])\n",
    "# print(\"Num unique sightings: \", len(unique_complete_hotspot_sightings_df))\n",
    "# print(\"Num solo sightings: \", unique_complete_hotspot_sightings_df['GROUP IDENTIFIER'].isna().sum().compute())\n",
    "# print(\"Num species: \", unique_complete_hotspot_sightings_df['COMMON NAME'].nunique().compute())\n",
    "# print(\"Num hotspots: \", unique_complete_hotspot_sightings_df['LOCALITY ID'].nunique().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd6d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklists_df = dd.read_csv('data/ebd_US-VT_smp_relJul-2025_sampling.txt',\n",
    "                        sep='\\t', \n",
    "                        blocksize=25e6,\n",
    "                        usecols=['LAST EDITED DATE', 'OBSERVATION DATE', 'LOCALITY', 'LOCALITY ID', 'LOCALITY TYPE', 'SAMPLING EVENT IDENTIFIER', 'OBSERVATION TYPE', 'DURATION MINUTES', 'EFFORT DISTANCE KM', 'NUMBER OBSERVERS', 'ALL SPECIES REPORTED', 'GROUP IDENTIFIER'],\n",
    "                        dtype={\n",
    "                            'LAST EDITED DATE': 'string',\n",
    "                            'OBSERVATION DATE': 'period[D]',\n",
    "                            'LOCALITY': 'string',\n",
    "                            'LOCALITY ID': 'string',\n",
    "                            'LOCALITY TYPE': 'category',\n",
    "                            'SAMPLING EVENT IDENTIFIER': 'string',\n",
    "                            'OBSERVATION TYPE': 'category',\n",
    "                            'DURATION MINUTES': 'UInt16',\n",
    "                            'EFFORT DISTANCE KM': 'Float32',\n",
    "                            'NUMBER OBSERVERS': 'UInt8',\n",
    "                            'ALL SPECIES REPORTED': 'boolean',\n",
    "                            'GROUP IDENTIFIER': 'string'\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "checklists_df['LAST EDITED DATE'] = dd.to_datetime(checklists_df['LAST EDITED DATE'], errors='coerce')\n",
    "checklists_df['LOCALITY ID'] = checklists_df['LOCALITY ID'].str.extract(r'(\\d+)$')[0].astype('Int64')\n",
    "checklists_df['SAMPLING EVENT IDENTIFIER'] = checklists_df['SAMPLING EVENT IDENTIFIER'].str.extract(r'(\\d+)$')[0].astype('Int64')\n",
    "sightings_df['GROUP IDENTIFIER'] = sightings_df['GROUP IDENTIFIER'].str.extract(r'(\\d+)$')[0].astype('Int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd90052",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_parquet(checklists_df, 'data/VT_checklists.parquet', engine=\"pyarrow\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf86360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklists_df = dd.read_parquet('data/VT_checklists.parquet', engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7effeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hotspot_checklists_df = checklists_df[\n",
    "      (checklists_df['ALL SPECIES REPORTED'])\n",
    "    & (checklists_df['LOCALITY TYPE'] == 'H')\n",
    "]\n",
    "\n",
    "solo_checklists_df = complete_hotspot_checklists_df[complete_hotspot_checklists_df['GROUP IDENTIFIER'].isna()]\n",
    "group_checklists_df = complete_hotspot_checklists_df[complete_hotspot_checklists_df['GROUP IDENTIFIER'].notnull()]\n",
    "unique_group_checklists_df = group_checklists_df.drop_duplicates(subset=['GROUP IDENTIFIER'])\n",
    "unique_complete_hotspot_checklists_df = dd.concat([solo_checklists_df, unique_group_checklists_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7051e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total checklists per day at each hotspot\n",
    "total_checklists_per_day_df = (\n",
    "    unique_complete_hotspot_checklists_df\n",
    "    .groupby(['LOCALITY ID', 'OBSERVATION DATE'], observed=True)\n",
    "    .size()\n",
    "    .rename('TOTAL CHECKLISTS')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "total_sightings_per_day_df = (\n",
    "    unique_complete_hotspot_sightings_df\n",
    "    .groupby(['LOCALITY ID', 'OBSERVATION DATE', 'COMMON NAME'], observed=True)\n",
    "    ['SAMPLING EVENT IDENTIFIER']\n",
    "    .nunique()\n",
    "    .rename('TOTAL SIGHTINGS')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "abundance_df = total_sightings_per_day_df.merge(\n",
    "    total_checklists_per_day_df,\n",
    "    on=['LOCALITY ID', 'OBSERVATION DATE'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d729553b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191368\n",
      "2829175\n"
     ]
    }
   ],
   "source": [
    "print(len(total_checklists_per_day_df))\n",
    "print(len(total_sightings_per_day_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9d7dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL CHECKLISTS\n",
      "18.0        332\n",
      "3.0      174638\n",
      "26.0         30\n",
      "17.0        196\n",
      "2.0      494192\n",
      "24.0         58\n",
      "35.0         35\n",
      "6.0       18608\n",
      "27.0         34\n",
      "7.0       13676\n",
      "4.0       78414\n",
      "15.0        488\n",
      "33.0         36\n",
      "5.0       40770\n",
      "1.0     1987948\n",
      "11.0       1736\n",
      "14.0        509\n",
      "25.0         91\n",
      "8.0        6630\n",
      "13.0        758\n",
      "16.0        300\n",
      "12.0       1180\n",
      "21.0         51\n",
      "19.0         57\n",
      "10.0       3139\n",
      "9.0        4817\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(abundance_df['TOTAL CHECKLISTS'].value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abundance_df['COMMON NAME'] = abundance_df['COMMON NAME'].cat.as_known()\n",
    "abundance_df['TOTAL CHECKLISTS'] = abundance_df['TOTAL CHECKLISTS'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "631d945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abundance_df['ABUNDANCE'] = abundance_df['TOTAL SIGHTINGS'] / abundance_df['TOTAL CHECKLISTS']\n",
    "abundance_df['DAY OF YEAR'] = abundance_df['OBSERVATION DATE'].dt.dayofyear\n",
    "\n",
    "mean_abundance_df = (\n",
    "    abundance_df\n",
    "    .groupby(['LOCALITY ID', 'COMMON NAME', 'DAY OF YEAR'], observed=True)\n",
    "    ['ABUNDANCE']\n",
    "    .mean()\n",
    "    .rename('MEAN ABUNDANCE')\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd2e4541",
   "metadata": {},
   "outputs": [
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntCastingNaNError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mabundance_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/VT_abundance.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m mean_abundance_df.to_parquet(\u001b[33m'\u001b[39m\u001b[33mdata/VT_mean_abundance.parquet\u001b[39m\u001b[33m'\u001b[39m, engine=\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m, write_index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nickk\\Github\\listr\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:3321\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m   3318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, **kwargs):\n\u001b[32m   3319\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdask_expr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nickk\\Github\\listr\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\io\\parquet.py:661\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, engine, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m         out = new_collection(\n\u001b[32m    638\u001b[39m             ToParquet(\n\u001b[32m    639\u001b[39m                 df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    657\u001b[39m             )\n\u001b[32m    658\u001b[39m         )\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     out = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[32m    666\u001b[39m fs.invalidate_cache(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nickk\\Github\\listr\\.venv\\Lib\\site-packages\\dask\\base.py:373\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    350\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nickk\\Github\\listr\\.venv\\Lib\\site-packages\\dask\\base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nickk\\Github\\listr\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:145\u001b[39m, in \u001b[36m_astype_float_to_int_nansafe\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(values).all():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values >= \u001b[32m0\u001b[39m).all():\n",
      "\u001b[31mIntCastingNaNError\u001b[39m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "abundance_df.to_parquet('data/VT_abundance.parquet', engine=\"pyarrow\", write_index=False)\n",
    "mean_abundance_df.to_parquet('data/VT_mean_abundance.parquet', engine=\"pyarrow\", write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
